tweets_corpus_US <- Corpus(VectorSource(rt_df_US$text))
tweets_corpus_AUS <- Corpus(VectorSource(rt_df_AUS$text))
tweets_corpus_UK <- Corpus(VectorSource(rt_df_UK$text))
tm_map(tweets_corpus_US, removePunctuation) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, removePunctuation) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, removePunctuation) -> tweets_corpus_UK
tm_map(tweets_corpus_US, removeNumbers) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, removeNumbers) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, removeNumbers) -> tweets_corpus_UK
tm_map(tweets_corpus_US, stripWhitespace) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, stripWhitespace) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, stripWhitespace) -> tweets_corpus_UK
tm_map(tweets_corpus_US, tolower) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, tolower) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, tolower) -> tweets_corpus_UK
tm_map(tweets_corpus_US, removeWords, stopwords("english")) -> tweets_corpus_US
tweet_dtm_US <- DocumentTermMatrix(tweets_corpus_US)
tm_map(tweets_corpus_US, removePunctuation) -> tweets_corpus_US
library(tm)
tm_map(tweets_corpus_US, removePunctuation) -> tweets_corpus_US
gsub("!","","mmm")
gsub("!","","!mmm")
gsub("!","","!mm!!!!m")
gsub("!m","","!mm!!!!m")
gsub("m","","!mm!!!!m")
gsub("!@#","","!mm!!!!m@#$")
gsub("!*@*#*","","!mm!!!!m@#$")
gsub("!*@*#*\\*","","!mm!!!!m@#$*")
gsub("!*@*#*\*","","!mm!!!!m@#$*")
gsub("!*@*#*","","#@!jbjkbwee")
gsub("^aeiou","","#@!jbjkbwee")
going <- "a1~!@#$%^&*bcd(){}_+:efg\"<>?,./;'[]-="
gsub(pattern = "[[:punct:]]+",replacement = "",x = going)
{
gsub(pattern = "[[:punct:]]+",replacement = "",x = j)
}
{
gsub(pattern = "[[:punct:]]+",replacement = "",x = j)
}
tweets_corpus_US[1]
tweets_corpus_US[2]
tweets_corpus_US[3]
tweets_corpus_US
tweets_corpus_US[1][1]
View(tweets_corpus_AUS)
tweets_corpus_US[][3]
tweets_corpus_US[[3]]
tweets_corpus_US[[1]]
tweets_corpus_US[[2]]
tweets_corpus_US[[3]]
tweets_corpus_US[[4]]
tweets_corpus_US[[3]][1]
tweets_corpus_US[[3]][2]
tweets_corpus_US[[3]]$content
tweets_corpus_US[[1]]$content
tweets_corpus_US[[2]]$content
gsub(pattern = "[[:punc:]]+", "", tweets_corpus_US[[2]]$content)
gsub(pattern = "[[:punc:]]+", "", tweets_corpus_US[[1]]$content)
gsub(pattern = "[[:punc:]]+", "", tweets_corpus_US[[4]]$content)
gsub(pattern = "!?", "", tweets_corpus_US[[4]]$content)
gsub(pattern = "[:punc:]", "", tweets_corpus_US[[4]]$content)
gsub(pattern = "[[:punc:]]", "", tweets_corpus_US[[4]]$content)
gsub('[^\x20-\x7E]', '', tweets_corpus_US[[3]]$content)
gsub('[^\x20-\x7E]', '', tweets_corpus_US[[1]]$content)
gsub('[^\x20-\x7E]', '', tweets_corpus_US[[2]]$content)
iconv(tweets_corpus_US[[1]]$content, 'utf-8', 'ascii', sub='')
tweets_corpus_US[[1]]$content
grep("!*", "!mm!")
grep("!*", "!mm!", value = FALSE)
grep("!*", "!mm!", value = TRUE)
grep("!*", "!mm!", value = TRUE, perl = TRUE)
grep("!*", "!mm!", value = TRUE, perl = FALSE)
iconv(tweets_corpus_US[[1]]$content, 'utf-8', 'ascii', sub='')
{
tweets_corpus_US[[j]]$content  <- iconv(tweets_corpus_US[[j]]$content, 'utf-8', 'ascii', sub='')
}
seq(tweets_corpus_US)
tweets_corpus_US
tweets_corpus_US$`1`
tweets_corpus_US$`3`
tweets_corpus_US[[1]]
tweets_corpus_US[[2]]
tweets_corpus_US[[3]]
tweets_corpus_US[[3]]$meta
length(tweets_corpus_US)
{
tweets_corpus_US[[j]]$content  <- iconv(tweets_corpus_US[[j]]$content, 'utf-8', 'ascii', sub='')
}
for(j  in length(tweets_corpus_US)) {}
for(j  in length(tweets_corpus_US)) { j }
for(j  in length(tweets_corpus_US)) { print(j) }
for(j  in 1..length(tweets_corpus_US)) { print(j) }
for(j  in 1..10)) { print(j) }
for(j  in 1 to 10)) { print(j) }
rt_df$text[1]
rt_df$text[2]
{
rt_df$text[[j]]  <- iconv(rt_df$text[[j]], 'utf-8', 'ascii', sub='')
}
rt_df$text[1]
rt_df$text[2]
{
rt_df$text[[j]]  <- iconv(rt_df$text[[j]], 'utf-8', 'ascii', sub='')
}
for(j in 1:length(rt_df$text))
{
rt_df$text[[j]]  <- iconv(rt_df$text[[j]], 'utf-8', 'ascii', sub='')
}
rt_df$text[1]
rt_df$text[2]
write.csv(rt_df, file = "rt_df.csv")
subset(rt_df, country == 'United States') -> rt_df_US
subset(rt_df, country == 'Australia') -> rt_df_AUS
subset(rt_df, country == 'United Kingdom') -> rt_df_UK
tweets_corpus_US <- Corpus(VectorSource(rt_df_US$text))
library(tm)
tweets_corpus_US <- Corpus(VectorSource(rt_df_US$text))
tweets_corpus_AUS <- Corpus(VectorSource(rt_df_AUS$text))
tweets_corpus_UK <- Corpus(VectorSource(rt_df_UK$text))
tm_map(tweets_corpus_US, removePunctuation) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, removePunctuation) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, removePunctuation) -> tweets_corpus_UK
tm_map(tweets_corpus_US, removeNumbers) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, removeNumbers) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, removeNumbers) -> tweets_corpus_UK
tm_map(tweets_corpus_US, stripWhitespace) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, stripWhitespace) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, stripWhitespace) -> tweets_corpus_UK
tm_map(tweets_corpus_US, tolower) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, tolower) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, tolower) -> tweets_corpus_UK
rt1 <- search_tweets(
"#vegan near: \"Dallas, TX\" within:15mi since:2011-01-01 until:2018-12-31", n = 250000, include_rts = FALSE, retryonratelimit = 900
)
## load rtweet package
library(rtweet)
create_token(
app = "vegan_sentiments_research",
api_key <- "nOCW4bthRbBfxq3Qeoh611oZH",
api_secret <- "Pq0gq4T2mvuI86WM7dqFk7PeYHMly6OrlScLiLxpwcvjwEmngV",
access_token <- "111242412-NUf3IdSRPce9zFPeFyprkARpHBQMud8T3zc05ovF",
access_token_secret <- "Yenl00WQOOUewgPK4loWrJJ2J78WTz3Wbbszz55QF63eJ")
rt1 <- search_tweets(
"#vegan near: \"Dallas, TX\" within:15mi since:2011-01-01 until:2018-12-31", n = 250000, include_rts = FALSE, retryonratelimit = 900
)
rt1 <- search_tweets(
"#vegan since:2011-01-01 until:2018-12-31", n = 250000, include_rts = FALSE, retryonratelimit = 900
)
inner_join(joined_lex, Tidy_DTM_AU, by = c("word" = "term"))
library(tidytext)
tidy(DTM_AU) -> Tidy_DTM_AU
# Make a corpus
Corpus_AU <- Corpus(VectorSource(AUS_ds1$content))
get_user_data(AUS_ds1) -> AUS_user_data
load("C:/Users/vinit/Summer Project (Vegan)/Summer_proj-2019-01-23_17.57.RData")
inner_join(nrc_lex, afinn_lex) -> joined_lex
library(tidytext)
tidy(DTM_AU) -> Tidy_DTM_AU
# Get the three lexicons
get_sentiments("nrc") -> nrc_lex
get_sentiments("bing") -> bing_lex
get_sentiments("afinn") -> afinn_lex
# Combine interested lexicons
library(dplyr)
inner_join(nrc_lex, afinn_lex) -> joined_lex
# Combine with the tidy DTM you made before
inner_join(joined_lex, Tidy_DTM_AU, by = c("word" = "term"))
# Combine with the tidy DTM you made before
inner_join(joined_lex, Tidy_DTM_AU, by = c("word" = "term")) -> Tidy_DTM_AU_Sentiments
View(Tidy_DTM_AU)
View(Tidy_DTM_AU_Sentiments)
View(Tidy_DTM_AU_Sentiments)
count(Tidy_DTM_AU_Sentiments, sentiment)
count(Tidy_DTM_AU_Sentiments, score)
library(qdap)
if(!requireNamespace('qdap')){
install.packages('qdap')
library(qdap)
} else
library(qdap)
polarity(Tidy_DTM_AU)
polarity(Tidy_DTM_AU, grouping.var = sentiment)
polarity(Tidy_DTM_AU, grouping.var = c("anger", "joy", "positive", "negative"))
save.image("C:/Users/vinit/Summer Project (Vegan)/Summer_proj-2019-01-24_14.51.RData")
#### Install and Load Libs necessary to access twitter ----
# This is comment for Logic B
if (!requireNamespace("twitteR")) {
install.packages("twitteR")
}
library(twitteR)
if (!requireNamespace("ROAuth")) {
install.packages("ROAuth")
}
library(ROAuth)
if (!requireNamespace("httr")) {
install.packages("httr")
}
library(httr)
# Set API Keys
auth_tw <- function() {
api_key <- "nwfZNhjGEgkR72YHVYrPUZnX9"
api_secret <- "Z4vVmEs6JqyiFe8BvTaTBAN3sQShVCbjMj3mcpwcqtCKGhuYhQ"
access_token <-
"111242412-bJYOp0m8i8M5YqXI4n2CKpYJEv2sPc0FYiUuQ7Vg"
access_token_secret <-
"zzuihiZgfJGWJOkm4GVBOlUdcsPrvwYNN1Oij4Mpo3lmV"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
}
auth_tw()
# function to remove special characters
removeSpecialChars <- function(x) {
gsub("[^a-zA-Z0-9 ]", " ", x)
}
# function to remove hyperlinks
removeUrl <- function(sentence) {
gsub("(http[^ ]*)", "", sentence)
}
library(stringr) # for str_count
# function to count urls
countUrl <- function(sentence) {
str_count(sentence, "http[^ ]*")
}
# function to count hastags
countHashtag <- function(sentence) {
str_count(sentence, "#[^ ]*")
}
# function to get user information
get_user_data <- function(csv_ds) {
# Get more information about the users
usernames <-
csv_ds$handle #name has strange characters. Better option is to
# search using the handle variable. Only need to remove the @ char in handle
as.character(usernames) -> usernames
gsub("@", "", usernames) -> usernames
unique(usernames) -> usernames
# Creating structure of data frame for storing user metadata
twListToDF(lookupUsers("vinitp1402")) -> user_metadata
user_metadata[-1, ] -> user_metadata
for (j in 1:length(usernames)) {
user <- usernames[j]
print(paste0("Processing ", j, " of ", length(usernames), ":", user))
# Get data for each user and store in a temp data frame
twListToDF(lookupUsers(user)) -> new_df
# Append the new data to the user_metadata data frame
rbind(user_metadata, new_df) -> user_metadata
}
print("Done extracting all user data...")
return(user_metadata)
}
find_freq_words <- function(x) {
# x = tidy data frame
inner_join(x, lex_nrc, by = c("term" = "word")) %>%
filter(sentiment == x) %>%
count(term, sort = TRUE) %>%
top_n(n = 5) %>%
arrange(desc(n)) -> tempp_tibble
unlist(tempp_tibble$term) %>%
paste(collapse = " ")
}
# Read source files ----
setwd("C:/Users/vinit/Summer Project (Vegan)/src_files_proj")
read.csv("csv_files/2011.csv") -> "tweets_2011" #processed
read.csv("csv_files/2013.csv") -> "tweets_2013" #processed
View(tweets_2011)
lookupUsers("vinitp1402")
lookupUsers("vinitp1402") -> vinit
View(vinit)
auth_tw()
#### Install and Load Libs necessary to access twitter ----
# This is comment for Logic B
if (!requireNamespace("twitteR")) {
install.packages("twitteR")
}
library(twitteR)
if (!requireNamespace("ROAuth")) {
install.packages("ROAuth")
}
library(ROAuth)
if (!requireNamespace("httr")) {
install.packages("httr")
}
library(httr)
# Set API Keys
auth_tw <- function() {
api_key <- "nwfZNhjGEgkR72YHVYrPUZnX9"
api_secret <- "Z4vVmEs6JqyiFe8BvTaTBAN3sQShVCbjMj3mcpwcqtCKGhuYhQ"
access_token <-
"111242412-bJYOp0m8i8M5YqXI4n2CKpYJEv2sPc0FYiUuQ7Vg"
access_token_secret <-
"zzuihiZgfJGWJOkm4GVBOlUdcsPrvwYNN1Oij4Mpo3lmV"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
}
auth_tw()
# function to remove special characters
removeSpecialChars <- function(x) {
gsub("[^a-zA-Z0-9 ]", " ", x)
}
# function to remove hyperlinks
removeUrl <- function(sentence) {
gsub("(http[^ ]*)", "", sentence)
}
library(stringr) # for str_count
# function to count urls
countUrl <- function(sentence) {
str_count(sentence, "http[^ ]*")
}
# function to count hastags
countHashtag <- function(sentence) {
str_count(sentence, "#[^ ]*")
}
# function to get user information
get_user_data <- function(csv_ds) {
# Get more information about the users
usernames <-
csv_ds$handle #name has strange characters. Better option is to
# search using the handle variable. Only need to remove the @ char in handle
as.character(usernames) -> usernames
gsub("@", "", usernames) -> usernames
unique(usernames) -> usernames
# Creating structure of data frame for storing user metadata
twListToDF(lookupUsers("vinitp1402")) -> user_metadata
user_metadata[-1, ] -> user_metadata
for (j in 1:length(usernames)) {
user <- usernames[j]
print(paste0("Processing ", j, " of ", length(usernames), ":", user))
# Get data for each user and store in a temp data frame
twListToDF(lookupUsers(user)) -> new_df
# Append the new data to the user_metadata data frame
rbind(user_metadata, new_df) -> user_metadata
}
print("Done extracting all user data...")
return(user_metadata)
}
find_freq_words <- function(x) {
# x = tidy data frame
inner_join(x, lex_nrc, by = c("term" = "word")) %>%
filter(sentiment == x) %>%
count(term, sort = TRUE) %>%
top_n(n = 5) %>%
arrange(desc(n)) -> tempp_tibble
unlist(tempp_tibble$term) %>%
paste(collapse = " ")
}
# Read source files ----
setwd("C:/Users/vinit/Summer Project (Vegan)/src_files_proj")
# Collect user's data
get_user_data(tweets_2011) -> user_2011_df
# Merge user data with tweets data ----
if (!requireNamespace('sqldf')) {
install.packages('sqldf')
}
library('sqldf')
View(tweets_2011)
View(user_2011_df)
# Remove the '@' in handle variable to perform a join with user data df
tweets_2011$handle <- gsub("@", "", tweets_2011$handle)
rownames(user_2011_df)
user_2011_df$handle <- rownames(user_2011_df)
View(user_2011_df)
merge(tweets_2011, user_2011_df)
merge(tweets_2011, user_2011_df, by = c("handle" == "handle"))
merge(tweets_2011, user_2011_df, by = "handle")
merge(tweets_2011, user_2011_df, by = "handle") -> tweets_2011
View(tweets_2011)
# Export this to csv (for client presentation)
write.csv(tweets_2011, file = "tweets_2011_master_table.csv", row.names = FALSE)
# counting hastags
# first add a new column
tweets_2011$no_of_hashtags <- 0
for (i in 1:length(tweets_2011$content)) {
tweets_2011$no_of_hashtags[i] <- countHashtag(tweets_2011$content[i])
}
# counting urls
# first add a new column
tweets_2011$no_of_urls <- 0
for (i in 1:length(tweets_2011$content)) {
tweets_2011$no_of_urls[i] <- countUrl(tweets_2011$content[i])
}
# remove urls
tweets_2011$content <- sapply(tweets_2011$content, removeUrl)
# remove special characters
tweets_2011$content <- sapply(tweets_2011$content, removeSpecialChars)
# convert all tweets to lower case
tweets_2011$content <- sapply(tweets_2011$content, tolower)
if (!requireNamespace('tm')) {
# for Corpus() function
install.packages('tm')
}
library('tm')
# Make a corpus
aus_corpus <- Corpus(VectorSource(tweets_2011$content))
# Convert to DTM
aus_dtm <- DocumentTermMatrix(aus_corpus)
# prepare a tidy data frame
# tidy data frame lists eveyr word with their frequency of occurence for every line
library(tidytext)
tidy(aus_dtm) -> aus_tidy_dtm
# Get the three lexicons
get_sentiments("nrc") -> lex_nrc
# arranging the sentiments
inner_join(aus_tidy_dtm, lex_nrc, by = c("term" = "word")) %>%
# group_by(document) %>%
count(sentiment, sort = TRUE) -> nrc_sentiments_aus
library(dplyr)
# arranging the sentiments
inner_join(aus_tidy_dtm, lex_nrc, by = c("term" = "word")) %>%
# group_by(document) %>%
count(sentiment, sort = TRUE) -> nrc_sentiments_aus
View(nrc_sentiments_aus)
# Find top keywords associated with each sentiment and store in dataframe
# Add a new column top words
nrc_sentiments_aus$top_words <- c("")
for (i in 1:length(nrc_sentiments_aus$sentiment)) {
find_freq_words(nrc_sentiments_aus$sentiment[i]) -> nrc_sentiments_aus$top_words[i]
}
for (i in 1:length(nrc_sentiments_aus$sentiment)) {
find_freq_words(nrc_sentiments_aus$sentiment[i]) -> nrc_sentiments_aus$top_words[i]
}
for (i in 1:length(nrc_sentiments_aus$sentiment)) {
find_freq_words(nrc_sentiments_aus$sentiment[i]) -> nrc_sentiments_aus$top_words[i]
}
View(aus_tidy_dtm)
View(nrc_sentiments_aus)
View(nrc_sentiments_aus)
View(lex_nrc)
View(aus_tidy_dtm)
View(lex_nrc)
find_freq_words <- function(x) {
# x = tidy data frame
inner_join(x, lex_nrc, by = c("term" == "word")) %>%
filter(sentiment == x) %>%
count(term, sort = TRUE) %>%
top_n(n = 5) %>%
arrange(desc(n)) -> tempp_tibble
unlist(tempp_tibble$term) %>%
paste(collapse = " ")
}
for (i in 1:length(nrc_sentiments_aus$sentiment)) {
find_freq_words(nrc_sentiments_aus$sentiment[i]) -> nrc_sentiments_aus$top_words[i]
}
inner_join(aus_tidy_dtm, lex_nrc, by = c("term" == "word")) %>%
filter(sentiment == "positive") %>%
count(term, sort = TRUE) %>%
top_n(n = 5) %>%
arrange(desc(n)) -> tempp_tibble
inner_join(aus_tidy_dtm, lex_nrc, by = c("term" = "word")) %>%
filter(sentiment == "positive") %>%
count(term, sort = TRUE) %>%
top_n(n = 5) %>%
arrange(desc(n)) -> tempp_tibble
View(tempp_tibble)
find_freq_words <- function(x, y) {
# x = tidy data frame
# y = sentiment
inner_join(x, lex_nrc, by = c("term" = "word")) %>%
filter(sentiment == y) %>%
count(term, sort = TRUE) %>%
top_n(n = 5) %>%
arrange(desc(n)) -> tempp_tibble
unlist(tempp_tibble$term) %>%
paste(collapse = " ")
}
# Find top keywords associated with each sentiment and store in dataframe
# Add a new column top words
nrc_sentiments_aus$top_words <- c("")
View(nrc_sentiments_aus)
for (i in 1:length(nrc_sentiments_aus$sentiment)) {
find_freq_words(aus_tidy_dtm, nrc_sentiments_aus$sentiment[i]) -> nrc_sentiments_aus$top_words[i]
}
View(nrc_sentiments_aus)
inner_join(aus_tidy_dtm, lex_nrc, by = c("term" = "word")) %>%
# group_by(document) %>%
filter(sentiment == "positive") %>%
count(term) %>%
top_n(n = 5, wt = n) %>%
arrange(desc(n))
library(qdap)
polarity(tweets_2011$content, tweets_2011$city)
View(tweets_2011)
View(nrc_sentiments_aus)
View(nrc_sentiments_aus)
View(tempp_tibble)
View(nrc_sentiments_aus)
View(tweets_2011)
tweets_2011$name.x <- NULL
tweets_2011$web.scraper.order <- NULL
tweets_2011$web.scraper.start.url <- NULL
tweets_2011$name.x <- NULL
tweets_2011$unix_timestamp <- NULL
tweets_2011$url.x <- NULL
colnames(tweets_2011$description) <- "user_description"
names(tweets_2011)[names(tweets_2011)=="description"]
names(tweets_2011)[names(tweets_2011)=="description"] <- "user_description"
View(tweets_2011)
tweets_2011$url.y <- NULL
tweets_2011$name.y <- NULL
tweets_2011$listedCount
View(tweets_2011)
tweets_2011$profileImageUrl <- NULL
# Export files necessary to explain client (only for 2019-02-05)
write.csv(nrc_sentiments_aus, file = "sentiments_australia.csv", row.names = FALSE)
write.csv(tweets_2011, file = "tweets_2011_master_table.csv", row.names = FALSE)
names(tweets_2011)[names(tweets_2011)=="created"] <- "account_created"
write.csv(tweets_2011, file = "tweets_2011_master_table.csv", row.names = FALSE)
names(tweets_2011)[names(tweets_2011)=="replies"] <- "tweet_replies"
names(tweets_2011)[names(tweets_2011)=="retweets"] <- "tweet_retweets"
names(tweets_2011)[names(tweets_2011)=="favorites"] <- "tweets_favorites"
names(tweets_2011)[names(tweets_2011)=="statusesCount"] <- "total_status_counts"
names(tweets_2011)[names(tweets_2011)=="followersCount"] <- "total_followers"
names(tweets_2011)[names(tweets_2011)=="favoritesCount"] <- "total_favorites"
names(tweets_2011)[names(tweets_2011)=="friendsCount"] <- "total_friends"
write.csv(tweets_2011, file = "tweets_2011_master_table.csv", row.names = FALSE)
