rt_df$text[1]
rt_df$text[2]
write.csv(rt_df, file = "rt_df.csv")
subset(rt_df, country == 'United States') -> rt_df_US
subset(rt_df, country == 'Australia') -> rt_df_AUS
subset(rt_df, country == 'United Kingdom') -> rt_df_UK
tweets_corpus_US <- Corpus(VectorSource(rt_df_US$text))
library(tm)
tweets_corpus_US <- Corpus(VectorSource(rt_df_US$text))
tweets_corpus_AUS <- Corpus(VectorSource(rt_df_AUS$text))
tweets_corpus_UK <- Corpus(VectorSource(rt_df_UK$text))
tm_map(tweets_corpus_US, removePunctuation) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, removePunctuation) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, removePunctuation) -> tweets_corpus_UK
tm_map(tweets_corpus_US, removeNumbers) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, removeNumbers) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, removeNumbers) -> tweets_corpus_UK
tm_map(tweets_corpus_US, stripWhitespace) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, stripWhitespace) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, stripWhitespace) -> tweets_corpus_UK
tm_map(tweets_corpus_US, tolower) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, tolower) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, tolower) -> tweets_corpus_UK
rt1 <- search_tweets(
"#vegan near: \"Dallas, TX\" within:15mi since:2011-01-01 until:2018-12-31", n = 250000, include_rts = FALSE, retryonratelimit = 900
)
## load rtweet package
library(rtweet)
create_token(
app = "vegan_sentiments_research",
api_key <- "nOCW4bthRbBfxq3Qeoh611oZH",
api_secret <- "Pq0gq4T2mvuI86WM7dqFk7PeYHMly6OrlScLiLxpwcvjwEmngV",
access_token <- "111242412-NUf3IdSRPce9zFPeFyprkARpHBQMud8T3zc05ovF",
access_token_secret <- "Yenl00WQOOUewgPK4loWrJJ2J78WTz3Wbbszz55QF63eJ")
rt1 <- search_tweets(
"#vegan near: \"Dallas, TX\" within:15mi since:2011-01-01 until:2018-12-31", n = 250000, include_rts = FALSE, retryonratelimit = 900
)
rt1 <- search_tweets(
"#vegan since:2011-01-01 until:2018-12-31", n = 250000, include_rts = FALSE, retryonratelimit = 900
)
inner_join(joined_lex, Tidy_DTM_AU, by = c("word" = "term"))
library(tidytext)
tidy(DTM_AU) -> Tidy_DTM_AU
# Make a corpus
Corpus_AU <- Corpus(VectorSource(AUS_ds1$content))
get_user_data(AUS_ds1) -> AUS_user_data
load("C:/Users/vinit/Summer Project (Vegan)/Summer_proj-2019-01-23_17.57.RData")
inner_join(nrc_lex, afinn_lex) -> joined_lex
library(tidytext)
tidy(DTM_AU) -> Tidy_DTM_AU
# Get the three lexicons
get_sentiments("nrc") -> nrc_lex
get_sentiments("bing") -> bing_lex
get_sentiments("afinn") -> afinn_lex
# Combine interested lexicons
library(dplyr)
inner_join(nrc_lex, afinn_lex) -> joined_lex
# Combine with the tidy DTM you made before
inner_join(joined_lex, Tidy_DTM_AU, by = c("word" = "term"))
# Combine with the tidy DTM you made before
inner_join(joined_lex, Tidy_DTM_AU, by = c("word" = "term")) -> Tidy_DTM_AU_Sentiments
View(Tidy_DTM_AU)
View(Tidy_DTM_AU_Sentiments)
View(Tidy_DTM_AU_Sentiments)
count(Tidy_DTM_AU_Sentiments, sentiment)
count(Tidy_DTM_AU_Sentiments, score)
library(qdap)
if(!requireNamespace('qdap')){
install.packages('qdap')
library(qdap)
} else
library(qdap)
polarity(Tidy_DTM_AU)
polarity(Tidy_DTM_AU, grouping.var = sentiment)
polarity(Tidy_DTM_AU, grouping.var = c("anger", "joy", "positive", "negative"))
save.image("C:/Users/vinit/Summer Project (Vegan)/Summer_proj-2019-01-24_14.51.RData")
#install.packages('RSelenium')
library(RSelenium)
library(rvest)
search_url <- "https://twitter.com/search?l=en&q=%23vegan%20since%3A2012-05-01%20until%3A2013-08-31&src=typd"
rD <- rsDriver(port=4444L,browser="chrome")
remDr =rD[["client"]]
# Navigate to the oage you want. This is equivalent to entering url in browser and hitting enter
remDr$navigate(search_url)
# OPen the browser
remDr$open()
# Navigate to the oage you want. This is equivalent to entering url in browser and hitting enter
remDr$navigate(search_url)
date <- "31 Dec 2012"
while (date != '1 Dec 2012') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_node(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> date
print(paste0(date, "\n"))
}
i <- 1
while (date != '1 Dec 2012') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_node(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> date
i <- i + 1
print(paste0(date, "\n"))
}
date <- "31 Dec 2012"
i <- 1
while (date != '25 Aug 2013') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_node(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> date
i <- i + 1
print(paste0(date, "\n"))
}
library(RSelenium)
library(rvest)
rD <- rsDriver(port=4444L,browser="chrome")
remDr =rD[["client"]]
# Navigate to the oage you want. This is equivalent to entering url in browser and hitting enter
remDr$navigate(search_url)
date <- "31 Dec 2012"
i <- 1
while (date != '25 Aug 2013') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_nodes(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> dates
dates[length(dates)] -> date
i <- i + 1
print(paste0(date, "\n"))
}
# function to scrape tweets and related metadata
twitter_scraper <- function(search_url){
library(RSelenium)
library(rvest)
#start RSelenium
rD <- rsDriver(port=4444L,browser="chrome")
remDr =rD[["client"]]
# Open the browser
# remDr$open()
# Navigate to the oage you want. This is equivalent to entering url in browser and hitting enter
remDr$navigate(search_url)
# # Log into twitter. However, not necessary to login to scrape data as on 2019-02-07
#
# mailid<-remDr$findElement(using = 'css',  "[class = 'text-input email-input js-signin-email']")
# mailid$sendKeysToElement(list("myemail@gmail.com"))
#
# # Enter password
#
# password<-remDr$findElement(using = 'css', ".LoginForm-password .text-input")
# password$sendKeysToElement(list("password"))
#
# # Click Enter
#
# login <- remDr$findElement(using = 'css',".js-submit")
# login$clickElement()
#scroll down 5 times, waiting for the page to load at each time
# for(i in 1:100){
#   remDr$executeScript(paste("scroll(0,",i*10000,");"))
#   Sys.sleep(3)
# }
date <- "1 Jan 2019"
i <- 1
while (date != '1 Jan 2011') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_nodes(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> dates
dates[length(dates)] -> date
i <- i + 1
print(paste0(date, "\n"))
}
page_source <- remDr$getPageSource()
# Scrape the content
read_html(page_source[[1]]) %>%
html_nodes(".tweet-text") %>%
html_text() %>%
# Convert to character
as.character() -> content
# Scrape date of tweet
read_html(page_source[[1]]) %>%
html_nodes(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> tweet_date
# Scrape handle
read_html(page_source[[1]]) %>%
html_nodes(".show-popup-with-id") %>%
html_text() %>%
# Convert to character
as.character() -> handle
# Scrape username
read_html(page_source[[1]]) %>%
html_nodes(".show-popup-with-id") %>%
html_text() %>%
# Convert to character
as.character() -> user_name
# Scrape number of replies
read_html(page_source[[1]]) %>%
html_nodes(xpath='//*[@class="ProfileTweet-actionButton js-actionButton js-actionReply"]/span/span') %>%
html_text() %>%
# Convert to character
as.character() -> tweet_replies
# Scrape number of retweets
read_html(page_source[[1]]) %>%
html_nodes(xpath='//*[@class="ProfileTweet-actionButton  js-actionButton js-actionRetweet"]/span/span') %>%
html_text() %>%
# Convert to character
as.character() -> tweet_retweets
# Scrape number of favourites
read_html(page_source[[1]]) %>%
html_nodes(xpath='//*[@class="ProfileTweet-actionButton js-actionButton js-actionFavorite"]/span/span') %>%
html_text() %>%
# Convert to character
as.character() -> tweet_favourites
# Close the server
remDr$close()
rD$server$stop()
# Combine the columns to form a data frame
cbind(handle, content, tweet_date, tweet_replies, tweet_retweets, tweet_favourites) -> scraped_df
return(scraped_df)
}
search_tweets_2012 <- "https://twitter.com/search?l=en&q=%23vegan%20since%3A2011-01-01%20until%3A2018-12-31&src=typd"
search_tweets_all <- "https://twitter.com/search?l=en&q=%23vegan%20since%3A2011-01-01%20until%3A2018-12-31&src=typd"
twitter_scraper(search_tweets_all) -> tweets_all
while (date != '1 Jan 2011') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_nodes(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> dates
dates[length(dates)] -> date
i <- i + 1
print(paste0(date, "\n"))
}
while (date != '1 Jan 2011') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_nodes(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> dates
dates[length(dates)] -> date
i <- i + 1
print(paste0(date, "\n"))
}
date
library(lubridate)
ymd()
ymd("2012-01-01")
ymd("1 Jan 2012")
format(today, format = "%Y-%m-%d")
today
format(date, format = "%Y-%m-%d")
format(date, format = "%m-%m-%d")
as.Date("2012-01-01", "%m/%d/%Y")
as.Date("2012-01-01", "%Y-%m-%d")
class(as.Date("2012-01-01", "%Y-%m-%d"))
# Dates in date format for proper
start_date <- as.Date("2012-01-01", "%Y-%m-%d")
end_date <- as.Date("2018-12-31", "%Y-%m-%d")
start_date
start_date+1
start_date+dweeks()
start_date+months(1)
start_date+months(12)
end_date + months(1)
i <- 1
since <- start_date
since <- start_date
until <- start_date + months(1)
since
until
format(since)
format(since, "%Y-%m-%d")
ymd(since)
dmy()
dmy(since)
dmy(date)
format(today, format="%B %d %Y")
format(since, format="%B %d %Y")
format(since, format="%d %m %Y")
format(since, format="%d %M %Y")
format(since, format="%d %b %Y")
since_str <- format(since, format(="%d %b %Y"))
since_str <- format(since, format"%d %b %Y")
class(since)
format(since, format"%d %b %Y")
since_str <- format(since, format = "%d %b %Y")
format(since + months(1), format = "%d %b %Y")
format(since, format = "%d %b %Y")
until_str <- format(since + months(1), format = "%d %b %Y")
paste0("https://twitter.com/search?l=en&q=%23vegan%20since%3A", since_str, "%20until%3A", until_str, "&src=typd")
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23vegan%20since%3A", since, "%20until%3A", until, "&src=typd")
paste0("https://twitter.com/search?l=en&q=%23vegan%20since%3A", since, "%20until%3A", until, "&src=typd")
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23vegan%20since%3A",
since, "%20until%3A", until, "&src=typd")
until_str
until_str <- format(since + months(3), format = "%d %b %Y")
library(RSelenium)
library(rvest)
remoteDriver() -> remDr
remDr$open(silent = TRUE)
rD <- rsDriver(port=4444L, browser="chrome")
remDr =rD[["client"]]
# Close the server
remDr$close()
remDr$open()
# Close the server
remDr$close()
remDr$open()
# Close the server
remDr$close()
data.frame(handle=character(), content=character(), tweet_date=character(), tweet_replies=integer(),
tweet_retweets=integer(), tweet_favourites=integer()) -> scraped_df
View(scraped_df)
data.frame(handle=character(), username=character() content=character(), tweet_date=character(), tweet_replies=integer(),
tweet_retweets=integer(), tweet_favourites=integer()) -> scraped_df
data.frame(handle=character(), username=character(), content=character(), tweet_date=character(), tweet_replies=integer(),
tweet_retweets=integer(), tweet_favourites=integer()) -> scraped_df
View(scraped_df)
scrape <- function() {
# Scrape the content
read_html(page_source[[1]]) %>%
html_nodes(".tweet-text") %>%
html_text() %>%
# Convert to character
as.character() -> content
# Scrape date of tweet
read_html(page_source[[1]]) %>%
html_nodes(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> tweet_date
# Scrape handle
read_html(page_source[[1]]) %>%
html_nodes(".show-popup-with-id") %>%
html_text() %>%
# Convert to character
as.character() -> handle
# Scrape username
read_html(page_source[[1]]) %>%
html_nodes(".show-popup-with-id") %>%
html_text() %>%
# Convert to character
as.character() -> user_name
# Scrape number of replies
read_html(page_source[[1]]) %>%
html_nodes(xpath = '//*[@class="ProfileTweet-actionButton js-actionButton js-actionReply"]/span/span') %>%
html_text() %>%
# Convert to character
as.character() -> tweet_replies
# Scrape number of retweets
read_html(page_source[[1]]) %>%
html_nodes(xpath = '//*[@class="ProfileTweet-actionButton  js-actionButton js-actionRetweet"]/span/span') %>%
html_text() %>%
# Convert to character
as.character() -> tweet_retweets
# Scrape number of favourites
read_html(page_source[[1]]) %>%
html_nodes(xpath = '//*[@class="ProfileTweet-actionButton js-actionButton js-actionFavorite"]/span/span') %>%
html_text() %>%
# Convert to character
as.character() -> tweet_favourites
cbind(
handle,
username,
content,
tweet_date,
tweet_replies,
tweet_retweets,
tweet_favourites
) -> temp_df
return(temp_df)
}
require(rvest)
rD <- rsDriver(port=4444L,browser="chrome")
remDr =rD[["client"]]
# OPen the browser
remDr$open()
twitter_scraper(search_url)
as.Date("2011-02-12")
since_str <- format(since, format = "%d %b %Y")
end_date <- as.Date(until)
end_date <- as.Date(until)
end_date_str <- format(end_date, format = "%d %b %Y")
data.frame(handle=character(), username=character(), content=character(), tweet_date=character(), tweet_replies=integer(),
tweet_retweets=integer(), tweet_favourites=integer()) -> scraped_df
start_date+month()
start_date+month(1)
start_date+months(1)
# Search for one month of tweets
# Prepare the dates that can be used in the search url
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23", hashtag, "%20since%3A",
since, "%20until%3A", until, "&src=typd")
hashtag="#vegan"
# Search for one month of tweets
# Prepare the dates that can be used in the search url
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23", hashtag, "%20since%3A",
since, "%20until%3A", until, "&src=typd")
search_url
hashtag="vegan"
# Search for one month of tweets
# Prepare the dates that can be used in the search url
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23", hashtag, "%20since%3A",
since, "%20until%3A", until, "&src=typd")
search_url
continue_flag <- TRUE
as.Date(since)
as.Date(until)
until_str <- as.Date(until)
since_str + months(1)
class(since_str)
since_str <- as.Date(since)
class(since_str)
since_str + months(1)
# Prepare the dates that can be used in the search url
since_str <- format(since, format = "%d %b %Y")
until_date <- as.Date(until)
until_str <- format(since + months(3), format = "%d %b %Y")
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23vegan%20since%3A",
since_date, "%20until%3A", until_date, "&src=typd")
since_date <- as.Date(since)
until_date <- as.Date(until)
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23vegan%20since%3A",
since_date, "%20until%3A", until_date, "&src=typd")
search_url
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23", hashtag, "%20since%3A",
since_date, "%20until%3A", until_date, "&src=typd")
search_url
since_scroll <- since_date
until_scroll <- until_date
setwd("C:/Users/vinit/Summer Project (Vegan)/src_files_proj")
read.csv(src_files_proj/tweets_2011-18.csv) -> tweets_ds
read.csv("src_files_proj/tweets_2011-18.csv") -> tweets_ds
read.csv("csv_files/tweets_2011-18.csv") -> tweets_ds
View(tweets_ds)
read.csv("csv_files/tweets_2011-18.csv") -> csv_ds
usernames <-
csv_ds$handle #name has strange characters. Better option is to
# search using the handle variable. Only need to remove the @ char in handle
as.character(usernames) -> usernames
gsub("@", "", usernames) -> usernames
unique(usernames) -> usernames
if (!requireNamespace("twitteR")) {
install.packages("twitteR")
}
library(twitteR)
if (!requireNamespace("ROAuth")) {
install.packages("ROAuth")
}
library(ROAuth)
if (!requireNamespace("httr")) {
install.packages("httr")
}
library(httr)
lookupUsers("vinitp1402")
auth_tw <- function() {
api_key <- "nwfZNhjGEgkR72YHVYrPUZnX9"
api_secret <- "Z4vVmEs6JqyiFe8BvTaTBAN3sQShVCbjMj3mcpwcqtCKGhuYhQ"
access_token <-
"111242412-bJYOp0m8i8M5YqXI4n2CKpYJEv2sPc0FYiUuQ7Vg"
access_token_secret <-
"zzuihiZgfJGWJOkm4GVBOlUdcsPrvwYNN1Oij4Mpo3lmV"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
}
auth_tw()
lookupUsers("vinitp1402")
twListToDF(lookupUsers("vinitp1402")) -> user_metadata
c("vinitp1402", "vinitp1402", "vinitp1402") -> temp_vec
lookupUsers(temp_vec)
twListToDF(lookupUsers(temp_vec))
# Creating structure of data frame for storing user metadata
twListToDF(lookupUsers(temp_vec)) -> user_metadata
View(user_metadata)
# Creating structure of data frame for storing user metadata
twListToDF(lookupUsers(usernames)) -> user_metadata
write.csv(user_metadata, file = "user_metadata.csv", row.names = FALSE)
