load("C:/Users/vinit/Summer Project (Vegan)/Summer_proj-2019-01-23_17.57.RData")
inner_join(nrc_lex, afinn_lex) -> joined_lex
library(tidytext)
tidy(DTM_AU) -> Tidy_DTM_AU
# Get the three lexicons
get_sentiments("nrc") -> nrc_lex
get_sentiments("bing") -> bing_lex
get_sentiments("afinn") -> afinn_lex
# Combine interested lexicons
library(dplyr)
inner_join(nrc_lex, afinn_lex) -> joined_lex
# Combine with the tidy DTM you made before
inner_join(joined_lex, Tidy_DTM_AU, by = c("word" = "term"))
# Combine with the tidy DTM you made before
inner_join(joined_lex, Tidy_DTM_AU, by = c("word" = "term")) -> Tidy_DTM_AU_Sentiments
View(Tidy_DTM_AU)
View(Tidy_DTM_AU_Sentiments)
View(Tidy_DTM_AU_Sentiments)
count(Tidy_DTM_AU_Sentiments, sentiment)
count(Tidy_DTM_AU_Sentiments, score)
library(qdap)
if(!requireNamespace('qdap')){
install.packages('qdap')
library(qdap)
} else
library(qdap)
polarity(Tidy_DTM_AU)
polarity(Tidy_DTM_AU, grouping.var = sentiment)
polarity(Tidy_DTM_AU, grouping.var = c("anger", "joy", "positive", "negative"))
save.image("C:/Users/vinit/Summer Project (Vegan)/Summer_proj-2019-01-24_14.51.RData")
#install.packages('RSelenium')
library(RSelenium)
library(rvest)
search_url <- "https://twitter.com/search?l=en&q=%23vegan%20since%3A2012-05-01%20until%3A2013-08-31&src=typd"
rD <- rsDriver(port=4444L,browser="chrome")
remDr =rD[["client"]]
# Navigate to the oage you want. This is equivalent to entering url in browser and hitting enter
remDr$navigate(search_url)
# OPen the browser
remDr$open()
# Navigate to the oage you want. This is equivalent to entering url in browser and hitting enter
remDr$navigate(search_url)
date <- "31 Dec 2012"
while (date != '1 Dec 2012') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_node(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> date
print(paste0(date, "\n"))
}
i <- 1
while (date != '1 Dec 2012') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_node(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> date
i <- i + 1
print(paste0(date, "\n"))
}
date <- "31 Dec 2012"
i <- 1
while (date != '25 Aug 2013') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_node(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> date
i <- i + 1
print(paste0(date, "\n"))
}
library(RSelenium)
library(rvest)
rD <- rsDriver(port=4444L,browser="chrome")
remDr =rD[["client"]]
# Navigate to the oage you want. This is equivalent to entering url in browser and hitting enter
remDr$navigate(search_url)
date <- "31 Dec 2012"
i <- 1
while (date != '25 Aug 2013') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_nodes(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> dates
dates[length(dates)] -> date
i <- i + 1
print(paste0(date, "\n"))
}
# function to scrape tweets and related metadata
twitter_scraper <- function(search_url){
library(RSelenium)
library(rvest)
#start RSelenium
rD <- rsDriver(port=4444L,browser="chrome")
remDr =rD[["client"]]
# Open the browser
# remDr$open()
# Navigate to the oage you want. This is equivalent to entering url in browser and hitting enter
remDr$navigate(search_url)
# # Log into twitter. However, not necessary to login to scrape data as on 2019-02-07
#
# mailid<-remDr$findElement(using = 'css',  "[class = 'text-input email-input js-signin-email']")
# mailid$sendKeysToElement(list("myemail@gmail.com"))
#
# # Enter password
#
# password<-remDr$findElement(using = 'css', ".LoginForm-password .text-input")
# password$sendKeysToElement(list("password"))
#
# # Click Enter
#
# login <- remDr$findElement(using = 'css',".js-submit")
# login$clickElement()
#scroll down 5 times, waiting for the page to load at each time
# for(i in 1:100){
#   remDr$executeScript(paste("scroll(0,",i*10000,");"))
#   Sys.sleep(3)
# }
date <- "1 Jan 2019"
i <- 1
while (date != '1 Jan 2011') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_nodes(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> dates
dates[length(dates)] -> date
i <- i + 1
print(paste0(date, "\n"))
}
page_source <- remDr$getPageSource()
# Scrape the content
read_html(page_source[[1]]) %>%
html_nodes(".tweet-text") %>%
html_text() %>%
# Convert to character
as.character() -> content
# Scrape date of tweet
read_html(page_source[[1]]) %>%
html_nodes(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> tweet_date
# Scrape handle
read_html(page_source[[1]]) %>%
html_nodes(".show-popup-with-id") %>%
html_text() %>%
# Convert to character
as.character() -> handle
# Scrape username
read_html(page_source[[1]]) %>%
html_nodes(".show-popup-with-id") %>%
html_text() %>%
# Convert to character
as.character() -> user_name
# Scrape number of replies
read_html(page_source[[1]]) %>%
html_nodes(xpath='//*[@class="ProfileTweet-actionButton js-actionButton js-actionReply"]/span/span') %>%
html_text() %>%
# Convert to character
as.character() -> tweet_replies
# Scrape number of retweets
read_html(page_source[[1]]) %>%
html_nodes(xpath='//*[@class="ProfileTweet-actionButton  js-actionButton js-actionRetweet"]/span/span') %>%
html_text() %>%
# Convert to character
as.character() -> tweet_retweets
# Scrape number of favourites
read_html(page_source[[1]]) %>%
html_nodes(xpath='//*[@class="ProfileTweet-actionButton js-actionButton js-actionFavorite"]/span/span') %>%
html_text() %>%
# Convert to character
as.character() -> tweet_favourites
# Close the server
remDr$close()
rD$server$stop()
# Combine the columns to form a data frame
cbind(handle, content, tweet_date, tweet_replies, tweet_retweets, tweet_favourites) -> scraped_df
return(scraped_df)
}
search_tweets_2012 <- "https://twitter.com/search?l=en&q=%23vegan%20since%3A2011-01-01%20until%3A2018-12-31&src=typd"
search_tweets_all <- "https://twitter.com/search?l=en&q=%23vegan%20since%3A2011-01-01%20until%3A2018-12-31&src=typd"
twitter_scraper(search_tweets_all) -> tweets_all
while (date != '1 Jan 2011') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_nodes(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> dates
dates[length(dates)] -> date
i <- i + 1
print(paste0(date, "\n"))
}
while (date != '1 Jan 2011') {
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)
page_source <- remDr$getPageSource()
read_html(page_source[[1]]) %>%
html_nodes(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> dates
dates[length(dates)] -> date
i <- i + 1
print(paste0(date, "\n"))
}
date
library(lubridate)
ymd()
ymd("2012-01-01")
ymd("1 Jan 2012")
format(today, format = "%Y-%m-%d")
today
format(date, format = "%Y-%m-%d")
format(date, format = "%m-%m-%d")
as.Date("2012-01-01", "%m/%d/%Y")
as.Date("2012-01-01", "%Y-%m-%d")
class(as.Date("2012-01-01", "%Y-%m-%d"))
# Dates in date format for proper
start_date <- as.Date("2012-01-01", "%Y-%m-%d")
end_date <- as.Date("2018-12-31", "%Y-%m-%d")
start_date
start_date+1
start_date+dweeks()
start_date+months(1)
start_date+months(12)
end_date + months(1)
i <- 1
since <- start_date
since <- start_date
until <- start_date + months(1)
since
until
format(since)
format(since, "%Y-%m-%d")
ymd(since)
dmy()
dmy(since)
dmy(date)
format(today, format="%B %d %Y")
format(since, format="%B %d %Y")
format(since, format="%d %m %Y")
format(since, format="%d %M %Y")
format(since, format="%d %b %Y")
since_str <- format(since, format(="%d %b %Y"))
since_str <- format(since, format"%d %b %Y")
class(since)
format(since, format"%d %b %Y")
since_str <- format(since, format = "%d %b %Y")
format(since + months(1), format = "%d %b %Y")
format(since, format = "%d %b %Y")
until_str <- format(since + months(1), format = "%d %b %Y")
paste0("https://twitter.com/search?l=en&q=%23vegan%20since%3A", since_str, "%20until%3A", until_str, "&src=typd")
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23vegan%20since%3A", since, "%20until%3A", until, "&src=typd")
paste0("https://twitter.com/search?l=en&q=%23vegan%20since%3A", since, "%20until%3A", until, "&src=typd")
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23vegan%20since%3A",
since, "%20until%3A", until, "&src=typd")
until_str
until_str <- format(since + months(3), format = "%d %b %Y")
library(RSelenium)
library(rvest)
remoteDriver() -> remDr
remDr$open(silent = TRUE)
rD <- rsDriver(port=4444L, browser="chrome")
remDr =rD[["client"]]
# Close the server
remDr$close()
remDr$open()
# Close the server
remDr$close()
remDr$open()
# Close the server
remDr$close()
data.frame(handle=character(), content=character(), tweet_date=character(), tweet_replies=integer(),
tweet_retweets=integer(), tweet_favourites=integer()) -> scraped_df
View(scraped_df)
data.frame(handle=character(), username=character() content=character(), tweet_date=character(), tweet_replies=integer(),
tweet_retweets=integer(), tweet_favourites=integer()) -> scraped_df
data.frame(handle=character(), username=character(), content=character(), tweet_date=character(), tweet_replies=integer(),
tweet_retweets=integer(), tweet_favourites=integer()) -> scraped_df
View(scraped_df)
scrape <- function() {
# Scrape the content
read_html(page_source[[1]]) %>%
html_nodes(".tweet-text") %>%
html_text() %>%
# Convert to character
as.character() -> content
# Scrape date of tweet
read_html(page_source[[1]]) %>%
html_nodes(".js-short-timestamp") %>%
html_text() %>%
# Convert to character
as.character() -> tweet_date
# Scrape handle
read_html(page_source[[1]]) %>%
html_nodes(".show-popup-with-id") %>%
html_text() %>%
# Convert to character
as.character() -> handle
# Scrape username
read_html(page_source[[1]]) %>%
html_nodes(".show-popup-with-id") %>%
html_text() %>%
# Convert to character
as.character() -> user_name
# Scrape number of replies
read_html(page_source[[1]]) %>%
html_nodes(xpath = '//*[@class="ProfileTweet-actionButton js-actionButton js-actionReply"]/span/span') %>%
html_text() %>%
# Convert to character
as.character() -> tweet_replies
# Scrape number of retweets
read_html(page_source[[1]]) %>%
html_nodes(xpath = '//*[@class="ProfileTweet-actionButton  js-actionButton js-actionRetweet"]/span/span') %>%
html_text() %>%
# Convert to character
as.character() -> tweet_retweets
# Scrape number of favourites
read_html(page_source[[1]]) %>%
html_nodes(xpath = '//*[@class="ProfileTweet-actionButton js-actionButton js-actionFavorite"]/span/span') %>%
html_text() %>%
# Convert to character
as.character() -> tweet_favourites
cbind(
handle,
username,
content,
tweet_date,
tweet_replies,
tweet_retweets,
tweet_favourites
) -> temp_df
return(temp_df)
}
require(rvest)
rD <- rsDriver(port=4444L,browser="chrome")
remDr =rD[["client"]]
# OPen the browser
remDr$open()
twitter_scraper(search_url)
as.Date("2011-02-12")
since_str <- format(since, format = "%d %b %Y")
end_date <- as.Date(until)
end_date <- as.Date(until)
end_date_str <- format(end_date, format = "%d %b %Y")
data.frame(handle=character(), username=character(), content=character(), tweet_date=character(), tweet_replies=integer(),
tweet_retweets=integer(), tweet_favourites=integer()) -> scraped_df
start_date+month()
start_date+month(1)
start_date+months(1)
# Search for one month of tweets
# Prepare the dates that can be used in the search url
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23", hashtag, "%20since%3A",
since, "%20until%3A", until, "&src=typd")
hashtag="#vegan"
# Search for one month of tweets
# Prepare the dates that can be used in the search url
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23", hashtag, "%20since%3A",
since, "%20until%3A", until, "&src=typd")
search_url
hashtag="vegan"
# Search for one month of tweets
# Prepare the dates that can be used in the search url
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23", hashtag, "%20since%3A",
since, "%20until%3A", until, "&src=typd")
search_url
continue_flag <- TRUE
as.Date(since)
as.Date(until)
until_str <- as.Date(until)
since_str + months(1)
class(since_str)
since_str <- as.Date(since)
class(since_str)
since_str + months(1)
# Prepare the dates that can be used in the search url
since_str <- format(since, format = "%d %b %Y")
until_date <- as.Date(until)
until_str <- format(since + months(3), format = "%d %b %Y")
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23vegan%20since%3A",
since_date, "%20until%3A", until_date, "&src=typd")
since_date <- as.Date(since)
until_date <- as.Date(until)
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23vegan%20since%3A",
since_date, "%20until%3A", until_date, "&src=typd")
search_url
# Prepare search string
search_url <- paste0("https://twitter.com/search?l=en&q=%23", hashtag, "%20since%3A",
since_date, "%20until%3A", until_date, "&src=typd")
search_url
since_scroll <- since_date
until_scroll <- until_date
setwd("C:/Users/vinit/Summer Project (Vegan)/src_files_proj/csv_files")
read.csv("user_metadata_en.csv") -> csv_ds
View(csv_ds)
csv_ds$Country
as.character(csv_ds$Country) -> csv_ds$Country
csv_ds$Country[1]
csv_ds$Country["USA"]
csv_ds$Country["US"]
library(stringr)
for (i in csv_ds$Country) {
print(csv_ds$Country[i])
}
for (i in length(csv_ds$Country)) {
print(csv_ds$Country[i])
}
for (i in 1:length(csv_ds$Country)) {
print(csv_ds$Country[i])
}
str_detect("California, USA", "*USA")
str_detect("California, USA", '*USA')
str_detect("California, USA", *USA)
str_detect("California, USA", "[[:alpha]]")
str_detect("California, USA", "[[:alpha]], USA")
str_detect("California, UA", "[[:alpha]], USA")
str_detect("California, US", "[[:alpha]], USA")
str_detect("California, USA", "[[:alpha]], USA")
for (i in 1:length(csv_ds$Country)) {
print(csv_ds$Country[i])
if (str_detect("California, USA", "[[:alpha]], USA")) {
print(csv_ds$Country[i])
}
}
if (str_detect("California, USA", "[[:alpha]], USA") == TRUE) {
print(csv_ds$Country[i])
}
for (i in 1:length(csv_ds$Country)) {
print(csv_ds$Country[i])
if (str_detect("California, USA", "[[:alpha]], USA") == TRUE) {
print(csv_ds$Country[i])
}
}
for (i in 1:length(csv_ds$Country)) {
print(csv_ds$Country[i])
if (str_detect(csv_ds$Country[i], "[[:alpha]], USA") == TRUE) {
print(csv_ds$Country[i])
}
}
str_detect("Galway, Ireland", "[[:alpha]], USA")
for (i in 1:length(csv_ds$Country)) {
print(csv_ds$Country[i])
if (str_detect(csv_ds$Country[i], "[[:alpha]], USA") == TRUE) {
print(csv_ds$Country[i])
} else {
print("no match")
}
}
if (str_detect(csv_ds$Country[i], "[[:alpha]], USA") == TRUE) {
print(csv_ds$Country[i])
} else {
print("no match")
}
for (i in 1:length(csv_ds$Country)) {
# print(csv_ds$Country[i])
if (str_detect(csv_ds$Country[i], "[[:alpha]], USA") == TRUE) {
print(csv_ds$Country[i])
} else {
print("no match")
}
}
for (i in 1:length(csv_ds$Country)) {
# print(csv_ds$Country[i])
if (str_detect(csv_ds$Country[i], "[[:alpha]], USA") == TRUE) {
print(csv_ds$Country[i]) }
# } else {
#   print("no match")
# }
}
str_split("oklahama, USA", " ", n = 2)
str_split("oklahama, USA", ", ", n = 2)
}
for (i in 1:length(csv_ds$Country)) {
# print(csv_ds$Country[i])
if (str_detect(csv_ds$Country[i], "[[:alpha:]], USA") == TRUE) {
print(str_split(csv_ds$Country[i], ", ", n = 2) )
}
}
for (i in 1:length(csv_ds$Country)) {
# print(csv_ds$Country[i])
if (str_detect(csv_ds$Country[i], "[[:alpha:]], USA") == TRUE) {
print(str_split(csv_ds$Country[i], ", ", n = 2) ) -> csv_ds$Country1[i]
}
}
