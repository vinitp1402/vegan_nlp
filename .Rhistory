library(tm)
Corpus(VectorSource(tweets_text$text))
tweets_corpus <- Corpus(VectorSource(tweets_text$text))
View(tweets_corpus)
tweet_dtm <- DocumentTermMatrix(tweets_corpus)
tweet_dtm
str(tweet_dtm)
tm_map(tweets_corpus, removePunctuation) -> tweets_corpus
tm_map(tweets_corpus, removeNumbers) -> tweets_corpus
tm_map(tweets_corpus, stripWhitespace) -> tweets_corpus
tm_map(tweets_corpus, content_transformer(tolower)) -> tweets_corpus
stopwords("english")
tm_map(tweets_corpus, removeWords, stopwords("english")) -> tweets_corpus
tweet_dtm <- DocumentTermMatrix(tweets_corpus)
findFreqTerms(tweet_dtm, lowfreq=20)
findFreqTerms(tweet_dtm, lowfreq=50)
findFreqTerms(tweet_dtm, lowfreq=100)
tweet_matrix <- as.matrix(tweet_dtm)
View(rt_df)
tweet_matrix <- as.matrix(tweet_dtm)
View(tweets_corpus)
View(tweet_dtm)
tweets_corpus
tweets_corpus[1]
tweets_corpus[2]
tweets_corpus[1:2]
findFreqTerms(tweet_dtm, lowfreq=100)
save.image("~/vegan_trends_2018.12.14_11.19.RData")
tweets_corpus
View(rt_df)
subset(rt_df, country == "Canada")
subset(rt_df, country == "USA")
subset(rt_df, country == "US")
subset(rt_df, country == "United States of America")
subset(rt_df, country == "Australia")
subset(rt_df, country == "Australia") -> rt_df_AUS
rt_df$country
rt_df$country != NA
rt_df$country != 'NA'
rt_df$country == 'Australia'
levels(rt_df$country)
class(rt_df$country)
as.factor(rt_df$country)
as.factor(rt_df$country) -> rt_df$country
levels(rt_df$country)
if(!requireNamespace('rtweet')){
install.packages('rtweet')
library(rtweet)
} else
library(rtweet)
if(!requireNamespace('rtweet')){
install.packages('rtweet')
library(rtweet)
} else
library(rtweet)
if (!requireNamespace("devtools", quietly = TRUE)) {
install.packages("devtools", dependencies = TRUE)
}
library(devtools)
devtools::install_github("mkearney/rtweet")
create_token(
app = "vegan_sentiments_research",
api_key <- "nOCW4bthRbBfxq3Qeoh611oZH",
api_secret <- "Pq0gq4T2mvuI86WM7dqFk7PeYHMly6OrlScLiLxpwcvjwEmngV",
access_token <- "111242412-NUf3IdSRPce9zFPeFyprkARpHBQMud8T3zc05ovF",
access_token_secret <- "Yenl00WQOOUewgPK4loWrJJ2J78WTz3Wbbszz55QF63eJ")
rt <- search_tweets(
"#vegan", n = 250000, include_rts = FALSE, retryonratelimit = 900
)
data.frame(rt) -> rt_df
#Convert list within the dataframe to a column in the dataframe
sapply(rt_df$hashtags, FUN = paste,collapse = "-") -> rt_df$hashtags1
sapply(rt_df$symbols, FUN = paste,collapse = "-") -> rt_df$symbols1
sapply(rt_df$urls_url, FUN = paste,collapse = "-") -> rt_df$urls_url1
sapply(rt_df$urls_t.co, FUN = paste,collapse = "-") -> rt_df$urls_t.co1
sapply(rt_df$urls_expanded_url, FUN = paste,collapse = "-") -> rt_df$urls_expanded_url1
sapply(rt_df$media_url, FUN = paste,collapse = "-") -> rt_df$media_url1lac
sapply(rt_df$media_t.co, FUN = paste,collapse = "-") -> rt_df$media_t.co1
sapply(rt_df$media_expanded_url, FUN = paste,collapse = "-") -> rt_df$media_expanded_url1
sapply(rt_df$media_type, FUN = paste,collapse = "-") -> rt_df$media_type1
sapply(rt_df$mentions_user_id, FUN = paste,collapse = "-") -> rt_df$mentions_user_id1
sapply(rt_df$mentions_screen_name, FUN = paste,collapse = "-") -> rt_df$mentions_screen_name1
sapply(rt_df$geo_coords, FUN = paste,collapse = "-") -> rt_df$geo_coords1
sapply(rt_df$coords_coords, FUN = paste,collapse = "-") -> rt_df$coords_coords1
sapply(rt_df$bbox_coords, FUN = paste,collapse = "-") -> rt_df$bbox_coords1
rt_df$hashtags <- NULL
rt_df$symbols <- NULL
rt_df$urls_url <- NULL
rt_df$urls_t.co <- NULL
rt_df$urls_expanded_url <- NULL
rt_df$media_url <- NULL
rt_df$media_t.co <- NULL
rt_df$media_expanded_url <- NULL
rt_df$media_type <- NULL
rt_df$mentions_user_id <- NULL
rt_df$mentions_screen_name <- NULL
rt_df$geo_coords <- NULL
rt_df$coords_coords <- NULL
rt_df$bbox_coords <- NULL
rt_df$ext_media_url <- NULL
rt_df$ext_media_t.co <- NULL
rt_df$ext_media_expanded_url <- NULL
write.csv(rt_df, file = "rt_df.csv")
library(tm)
View(rt_df)
subset(rt_df, country == 'United States')
subset(rt_df, country == 'United States') -> rt_df_US
subset(rt_df, country == 'Australia')
subset(rt_df, country == 'Australia') -> rt_df_AUS
subset(rt_df, country == ''United Kingdom') -> rt_df_UK
subset(rt_df, country == 'United Kingdom') -> rt_df_UK
View(rt_df_AUS)
tweets_corpus_US <- Corpus(VectorSource(rt_df_US))
tweets_corpus_AUS <- Corpus(VectorSource(rt_df_AUS))
tweets_corpus_UK <- Corpus(VectorSource(rt_df_UK))
View(rt_df_AUS)
View(tweets_corpus_AUS)
tweets_corpus_US <- Corpus(VectorSource(rt_df_US$text))
tweets_corpus_AUS <- Corpus(VectorSource(rt_df_AUS$text))
tweets_corpus_UK <- Corpus(VectorSource(rt_df_UK$text))
View(tweets_corpus_AUS)
tweets_corpus_AUS
tm_map(tweets_corpus_US, removePunctuation) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, removePunctuation) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, removePunctuation) -> tweets_corpus_UK
tm_map(tweets_corpus_US, removePunctuation) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, removePunctuation) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, removePunctuation) -> tweets_corpus_UK
tm_map(tweets_corpus_US, removeNumbers) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, removeNumbers) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, removeNumbers) -> tweets_corpus_UK
tm_map(tweets_corpus_US, stripWhitespace) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, stripWhitespace) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, stripWhitespace) -> tweets_corpus_UK
tm_map(tweets_corpus_US, tolower) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, tolower) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, tolower) -> tweets_corpus_UK
tm_map(tweets_corpus_US, removeWords, stopwords("english")) -> tweets_corpus_US
tweet_dtm_US <- DocumentTermMatrix(tweets_corpus_US)
findFreqTerms(tweet_dtm_US, lowfreq=100)
tm_map(tweets_corpus_AUS, removeWords, stopwords("english")) -> tweets_corpus_AUS
tweet_dtm_AUS <- DocumentTermMatrix(tweets_corpus_AUS)
findFreqTerms(tweet_dtm_AUS, lowfreq=100)
tm_map(tweets_corpus_UK, removeWords, stopwords("english")) -> tweets_corpus_UK
tweet_dtm_UK <- DocumentTermMatrix(tweets_corpus_UK)
findFreqTerms(tweet_dtm_UK, lowfreq=100)
findFreqTerms(tweet_dtm_UK, lowfreq=100)
tweets_corpus_US <- Corpus(VectorSource(rt_df_US$text))
tweets_corpus_AUS <- Corpus(VectorSource(rt_df_AUS$text))
tweets_corpus_UK <- Corpus(VectorSource(rt_df_UK$text))
tm_map(tweets_corpus_US, removePunctuation) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, removePunctuation) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, removePunctuation) -> tweets_corpus_UK
tm_map(tweets_corpus_US, removeNumbers) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, removeNumbers) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, removeNumbers) -> tweets_corpus_UK
tm_map(tweets_corpus_US, stripWhitespace) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, stripWhitespace) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, stripWhitespace) -> tweets_corpus_UK
tm_map(tweets_corpus_US, tolower) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, tolower) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, tolower) -> tweets_corpus_UK
tm_map(tweets_corpus_US, removeWords, stopwords("english")) -> tweets_corpus_US
tweet_dtm_US <- DocumentTermMatrix(tweets_corpus_US)
tm_map(tweets_corpus_US, removePunctuation) -> tweets_corpus_US
library(tm)
tm_map(tweets_corpus_US, removePunctuation) -> tweets_corpus_US
gsub("!","","mmm")
gsub("!","","!mmm")
gsub("!","","!mm!!!!m")
gsub("!m","","!mm!!!!m")
gsub("m","","!mm!!!!m")
gsub("!@#","","!mm!!!!m@#$")
gsub("!*@*#*","","!mm!!!!m@#$")
gsub("!*@*#*\\*","","!mm!!!!m@#$*")
gsub("!*@*#*\*","","!mm!!!!m@#$*")
gsub("!*@*#*","","#@!jbjkbwee")
gsub("^aeiou","","#@!jbjkbwee")
going <- "a1~!@#$%^&*bcd(){}_+:efg\"<>?,./;'[]-="
gsub(pattern = "[[:punct:]]+",replacement = "",x = going)
{
gsub(pattern = "[[:punct:]]+",replacement = "",x = j)
}
{
gsub(pattern = "[[:punct:]]+",replacement = "",x = j)
}
tweets_corpus_US[1]
tweets_corpus_US[2]
tweets_corpus_US[3]
tweets_corpus_US
tweets_corpus_US[1][1]
View(tweets_corpus_AUS)
tweets_corpus_US[][3]
tweets_corpus_US[[3]]
tweets_corpus_US[[1]]
tweets_corpus_US[[2]]
tweets_corpus_US[[3]]
tweets_corpus_US[[4]]
tweets_corpus_US[[3]][1]
tweets_corpus_US[[3]][2]
tweets_corpus_US[[3]]$content
tweets_corpus_US[[1]]$content
tweets_corpus_US[[2]]$content
gsub(pattern = "[[:punc:]]+", "", tweets_corpus_US[[2]]$content)
gsub(pattern = "[[:punc:]]+", "", tweets_corpus_US[[1]]$content)
gsub(pattern = "[[:punc:]]+", "", tweets_corpus_US[[4]]$content)
gsub(pattern = "!?", "", tweets_corpus_US[[4]]$content)
gsub(pattern = "[:punc:]", "", tweets_corpus_US[[4]]$content)
gsub(pattern = "[[:punc:]]", "", tweets_corpus_US[[4]]$content)
gsub('[^\x20-\x7E]', '', tweets_corpus_US[[3]]$content)
gsub('[^\x20-\x7E]', '', tweets_corpus_US[[1]]$content)
gsub('[^\x20-\x7E]', '', tweets_corpus_US[[2]]$content)
iconv(tweets_corpus_US[[1]]$content, 'utf-8', 'ascii', sub='')
tweets_corpus_US[[1]]$content
grep("!*", "!mm!")
grep("!*", "!mm!", value = FALSE)
grep("!*", "!mm!", value = TRUE)
grep("!*", "!mm!", value = TRUE, perl = TRUE)
grep("!*", "!mm!", value = TRUE, perl = FALSE)
iconv(tweets_corpus_US[[1]]$content, 'utf-8', 'ascii', sub='')
{
tweets_corpus_US[[j]]$content  <- iconv(tweets_corpus_US[[j]]$content, 'utf-8', 'ascii', sub='')
}
seq(tweets_corpus_US)
tweets_corpus_US
tweets_corpus_US$`1`
tweets_corpus_US$`3`
tweets_corpus_US[[1]]
tweets_corpus_US[[2]]
tweets_corpus_US[[3]]
tweets_corpus_US[[3]]$meta
length(tweets_corpus_US)
{
tweets_corpus_US[[j]]$content  <- iconv(tweets_corpus_US[[j]]$content, 'utf-8', 'ascii', sub='')
}
for(j  in length(tweets_corpus_US)) {}
for(j  in length(tweets_corpus_US)) { j }
for(j  in length(tweets_corpus_US)) { print(j) }
for(j  in 1..length(tweets_corpus_US)) { print(j) }
for(j  in 1..10)) { print(j) }
for(j  in 1 to 10)) { print(j) }
rt_df$text[1]
rt_df$text[2]
{
rt_df$text[[j]]  <- iconv(rt_df$text[[j]], 'utf-8', 'ascii', sub='')
}
rt_df$text[1]
rt_df$text[2]
{
rt_df$text[[j]]  <- iconv(rt_df$text[[j]], 'utf-8', 'ascii', sub='')
}
for(j in 1:length(rt_df$text))
{
rt_df$text[[j]]  <- iconv(rt_df$text[[j]], 'utf-8', 'ascii', sub='')
}
rt_df$text[1]
rt_df$text[2]
write.csv(rt_df, file = "rt_df.csv")
subset(rt_df, country == 'United States') -> rt_df_US
subset(rt_df, country == 'Australia') -> rt_df_AUS
subset(rt_df, country == 'United Kingdom') -> rt_df_UK
tweets_corpus_US <- Corpus(VectorSource(rt_df_US$text))
library(tm)
tweets_corpus_US <- Corpus(VectorSource(rt_df_US$text))
tweets_corpus_AUS <- Corpus(VectorSource(rt_df_AUS$text))
tweets_corpus_UK <- Corpus(VectorSource(rt_df_UK$text))
tm_map(tweets_corpus_US, removePunctuation) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, removePunctuation) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, removePunctuation) -> tweets_corpus_UK
tm_map(tweets_corpus_US, removeNumbers) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, removeNumbers) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, removeNumbers) -> tweets_corpus_UK
tm_map(tweets_corpus_US, stripWhitespace) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, stripWhitespace) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, stripWhitespace) -> tweets_corpus_UK
tm_map(tweets_corpus_US, tolower) -> tweets_corpus_US
tm_map(tweets_corpus_AUS, tolower) -> tweets_corpus_AUS
tm_map(tweets_corpus_UK, tolower) -> tweets_corpus_UK
rt1 <- search_tweets(
"#vegan near: \"Dallas, TX\" within:15mi since:2011-01-01 until:2018-12-31", n = 250000, include_rts = FALSE, retryonratelimit = 900
)
## load rtweet package
library(rtweet)
create_token(
app = "vegan_sentiments_research",
api_key <- "nOCW4bthRbBfxq3Qeoh611oZH",
api_secret <- "Pq0gq4T2mvuI86WM7dqFk7PeYHMly6OrlScLiLxpwcvjwEmngV",
access_token <- "111242412-NUf3IdSRPce9zFPeFyprkARpHBQMud8T3zc05ovF",
access_token_secret <- "Yenl00WQOOUewgPK4loWrJJ2J78WTz3Wbbszz55QF63eJ")
rt1 <- search_tweets(
"#vegan near: \"Dallas, TX\" within:15mi since:2011-01-01 until:2018-12-31", n = 250000, include_rts = FALSE, retryonratelimit = 900
)
rt1 <- search_tweets(
"#vegan since:2011-01-01 until:2018-12-31", n = 250000, include_rts = FALSE, retryonratelimit = 900
)
inner_join(joined_lex, Tidy_DTM_AU, by = c("word" = "term"))
library(tidytext)
tidy(DTM_AU) -> Tidy_DTM_AU
# Make a corpus
Corpus_AU <- Corpus(VectorSource(AUS_ds1$content))
get_user_data(AUS_ds1) -> AUS_user_data
load("C:/Users/vinit/Summer Project (Vegan)/Summer_proj-2019-01-23_17.57.RData")
inner_join(nrc_lex, afinn_lex) -> joined_lex
library(tidytext)
tidy(DTM_AU) -> Tidy_DTM_AU
# Get the three lexicons
get_sentiments("nrc") -> nrc_lex
get_sentiments("bing") -> bing_lex
get_sentiments("afinn") -> afinn_lex
# Combine interested lexicons
library(dplyr)
inner_join(nrc_lex, afinn_lex) -> joined_lex
# Combine with the tidy DTM you made before
inner_join(joined_lex, Tidy_DTM_AU, by = c("word" = "term"))
# Combine with the tidy DTM you made before
inner_join(joined_lex, Tidy_DTM_AU, by = c("word" = "term")) -> Tidy_DTM_AU_Sentiments
View(Tidy_DTM_AU)
View(Tidy_DTM_AU_Sentiments)
View(Tidy_DTM_AU_Sentiments)
count(Tidy_DTM_AU_Sentiments, sentiment)
count(Tidy_DTM_AU_Sentiments, score)
library(qdap)
if(!requireNamespace('qdap')){
install.packages('qdap')
library(qdap)
} else
library(qdap)
polarity(Tidy_DTM_AU)
polarity(Tidy_DTM_AU, grouping.var = sentiment)
polarity(Tidy_DTM_AU, grouping.var = c("anger", "joy", "positive", "negative"))
save.image("C:/Users/vinit/Summer Project (Vegan)/Summer_proj-2019-01-24_14.51.RData")
# Merge userdata with tweets data
if (!requireNamespace('sqldf')) {
install.packages('sqldf')
}
library('sqldf')
#### Install and Load Libs necessary to access twitter ----
# Thhis is comment for LOgic B
if(!requireNamespace("twitteR")){
install.packages("twitteR")
}
library(twitteR)
if(!requireNamespace("ROAuth")){
install.packages("ROAuth")
}
library(ROAuth)
if(!requireNamespace("httr")){
install.packages("httr")
}
library(httr)
# Set API Keys
auth_tw <- function(){
api_key <- "nwfZNhjGEgkR72YHVYrPUZnX9"
api_secret <- "Z4vVmEs6JqyiFe8BvTaTBAN3sQShVCbjMj3mcpwcqtCKGhuYhQ"
access_token <- "111242412-bJYOp0m8i8M5YqXI4n2CKpYJEv2sPc0FYiUuQ7Vg"
access_token_secret <- "zzuihiZgfJGWJOkm4GVBOlUdcsPrvwYNN1Oij4Mpo3lmV"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
}
auth_tw()
# function to remove special characters
removeSpecialChars <- function(x)
gsub("[^a-zA-Z0-9 ]", " ", x)
# function to remove hyperlinks
removeUrl <- function(sentence) {
gsub("(http[^ ]*)", "", sentence)
}
library(stringr) # for str_count
# function to count urls
countUrl <- function(sentence) {
str_count(sentence, "http[^ ]*")
}
# function to count hastags
countHashtag <- function(sentence) {
str_count(sentence, "#[^ ]*")
}
# function to get user information
get_user_data <- function(csv_ds) {
# Get more information about the users
usernames <-
csv_ds$handle #name has strange characters. Better option is to
# search using the handle variable. Only need to remove the @ char in handle
as.character(usernames) -> usernames
gsub("@", "", usernames) -> usernames
unique(usernames) -> usernames
# Creating structure of data frame for storing user metadata
twListToDF(lookupUsers("vinitp1402")) -> user_metadata
user_metadata[-1,] -> user_metadata
for (j in 1:length(usernames)) {
user <- usernames[j]
print(paste0("Processing ", j, " of ", length(usernames), ":", user))
# Get data for each user and store in a temp data frame
twListToDF(lookupUsers(user)) -> new_df
# Append the new data to the user_metadata data frame
rbind(user_metadata, new_df) -> user_metadata
}
print("Done extracting all user data...")
return(user_metadata)
}
find_freq_words <- function(x) {
# x = tidy data frame
inner_join(x, lex_nrc, by = c("term" = "word")) %>%
filter(sentiment == x) %>%
count(term, sort = TRUE) %>%
top_n(n = 5) %>%
arrange(desc(n)) -> tempp_tibble
unlist(tempp_tibble$term) %>%
paste(collapse = " ")
}
# Read source files
setwd("C:/Users/vinit/Summer Project (Vegan)/csv_files")
# Read source files
setwd("C:/Users/vinit/Summer Project (Vegan)/src_files_proj")
read.csv("csv_files/2011.csv") -> 2011.csv
read.csv("csv_files/2011.csv") -> "2011.csv"
View(`2011.csv`)
#read.csv("csv_files/2012.csv") -> "2012.csv"
read.csv("csv_files/2013.csv") -> "2013.csv"
# Collect user's data
get_user_data("2011.csv") -> user_2011
# Collect user's data
get_user_data(2011.csv) -> user_2011
library('sqldf')
# Collect user's data
get_user_data("2011.csv") -> user_data_2011
#### Install and Load Libs necessary to access twitter ----
# This is comment for Logic B
if (!requireNamespace("twitteR")) {
install.packages("twitteR")
}
library(twitteR)
if (!requireNamespace("ROAuth")) {
install.packages("ROAuth")
}
library(ROAuth)
if (!requireNamespace("httr")) {
install.packages("httr")
}
library(httr)
# Set API Keys
auth_tw <- function() {
api_key <- "nwfZNhjGEgkR72YHVYrPUZnX9"
api_secret <- "Z4vVmEs6JqyiFe8BvTaTBAN3sQShVCbjMj3mcpwcqtCKGhuYhQ"
access_token <-
"111242412-bJYOp0m8i8M5YqXI4n2CKpYJEv2sPc0FYiUuQ7Vg"
access_token_secret <-
"zzuihiZgfJGWJOkm4GVBOlUdcsPrvwYNN1Oij4Mpo3lmV"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
}
auth_tw()
# function to remove special characters
removeSpecialChars <- function(x) {
gsub("[^a-zA-Z0-9 ]", " ", x)
}
# function to remove hyperlinks
removeUrl <- function(sentence) {
gsub("(http[^ ]*)", "", sentence)
}
library(stringr) # for str_count
# function to count urls
countUrl <- function(sentence) {
str_count(sentence, "http[^ ]*")
}
# function to count hastags
countHashtag <- function(sentence) {
str_count(sentence, "#[^ ]*")
}
# function to get user information
get_user_data <- function(csv_ds) {
# Get more information about the users
usernames <-
csv_ds$handle #name has strange characters. Better option is to
# search using the handle variable. Only need to remove the @ char in handle
as.character(usernames) -> usernames
gsub("@", "", usernames) -> usernames
unique(usernames) -> usernames
# Creating structure of data frame for storing user metadata
twListToDF(lookupUsers("vinitp1402")) -> user_metadata
user_metadata[-1, ] -> user_metadata
for (j in 1:length(usernames)) {
user <- usernames[j]
print(paste0("Processing ", j, " of ", length(usernames), ":", user))
# Get data for each user and store in a temp data frame
twListToDF(lookupUsers(user)) -> new_df
# Append the new data to the user_metadata data frame
rbind(user_metadata, new_df) -> user_metadata
}
print("Done extracting all user data...")
return(user_metadata)
}
find_freq_words <- function(x) {
# x = tidy data frame
inner_join(x, lex_nrc, by = c("term" = "word")) %>%
filter(sentiment == x) %>%
count(term, sort = TRUE) %>%
top_n(n = 5) %>%
arrange(desc(n)) -> tempp_tibble
unlist(tempp_tibble$term) %>%
paste(collapse = " ")
}
# Read source files ----
setwd("C:/Users/vinit/Summer Project (Vegan)/src_files_proj")
read.csv("csv_files/2011.csv") -> "tweets_2011"
View(tweets_2011)
read.csv("csv_files/2013.csv") -> "tweets_2013" #processed
# Collect user's data
get_user_data(tweets_2011) -> user_2011_df
View(user_2011_df)
get_user_data(tweets_2013) -> user_2013_df
View(tweets_2013)
get_user_data(tweets_2013) -> user_2013_df
get_user_data(tweets_2013) -> user_2013_df
View(tweets_2013)
